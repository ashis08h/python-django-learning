Apache spark -
Apache spark is a lightning-fast unified analytics engine for big data processing and machine learning.

100% open source under apache license
simple and easy to use api
in-memory processing engine.
distributed computing platform
unified engine which supports SQL, streaming, ML and graphing processing
integrates closely with other big data tools.

cluster - It is a backbone of apache spark. Here we can decide what are the nodes, what are the executors.

Inside Azure databricks we can see below things -
1) Clusters
2) Workspace/Notebook
3) Administration control
4) Optimized speed
5) Database/Table
6) Delta lake
7) SQL Analytics
8) ML Flow

All the above components are utilized by databricks.

Azure databricks can do below things
1) It has unified billing - means whenever cluster will run then only cost will cut. Once it stopped
cost will not come.

2) For messaging service we can use Azure IOT hub, Azure event hub,
3) It can integrate with Power BI, Azure ML, Azure data factory, Azure Devops, Azure active directory.
4) It can work with below data services
    a) Azure data Lake
    b) Azure blob storage
    c) Azure cosmos DB
    d) Azure SQL database
    e) Azure Synapse

What is the difference between Azure data factory and Azure databricks?

Azure Data Factory -
Purpose - ADF is primarily used for data integration services to perform ETL processes and orchestration
data movements at scale

Ease of use - ADF provides a drag-drop feature to create and maintain data pipelines visually.

Flexibility in coding - ADF faciliates the ETL pipeline process using GUI tools, developers have less
flexibility as they can not modify backend code.

Data processing - ADF does not support live streaming.

Azure Databricks -
Purpose - Databricks provides a collaboarative platform for data engineers and data scientists to perform
ETL as well as build machine learning models under single platform.

Ease of use - Databricks uses python, Spark, R, Java or SQL for performing Data Engineering and Data Science
activities using notebooks.

Flexibility in coding - Databricks implements a programmatic approach that provides the flexibility of fine
-tuning code to optimize performance

Data processing - Databricks supports both live and archive streaming options through Spark API.

Workspace - The workspace is the environment for accessing all your databricks assets. It organizes objects
(like: - Notebook, Libraries, Dashboards, and experiments) into folders and provides access to data objects
and computational resources.

Notebooks - A web-based interface to documents that contain runnable commands, visualization and narrative
text.

Folder - It's used to maintain the grouping and hierarchy.

Library - A package of code available to the notebook or job running on your cluster. Databricks runtimes
include many libraries, and you can add your own.

MLflow - A collection of MLflow runs for training a machine learning model.

Data -

Databricks file system (DBFS) - The databricks file system (DBFS) is a distributed file system mounted into an
Azure databricks workspace and available on azure databricks clusters.

Database - A collection of information that is organized so that it can be easiliy accessed, managed and updated.

Table - A representation of structured data, you query tables with Apache spark SQL and Apache SparkAPI's

Compute -
    Cluster - A set of computation resources and configurations on which you run notebooks and jobs.
    There are two types of clusters
    1) All purpose cluster.
    2) Job compute cluster.

Databricks runtime - The set of core components that run on the clusters managed by databricks, Databricks
offer several types of runtime.

    1) Databricks runtime with conda
    2) Databricks runtime for ML
    3) Databricks runtime for Genomics
    4) Databricks light

Authentications -
User - A unique individual who has access to the system
Group - A collection of user

Access Control List (ACL) - A list of permissions attached to the workspace, cluster, job, tables or ML flows.
An ACL specifies which users or system processes are guarantee access to the objects, as well as what operations
are allowed on the assets.

Cluster - A set of conputation resources and configurations on which you run notebooks and jobs.

VM - DRIVER
VM - WORKER, VM - WORKER, VM - WORKER

This whole is one 1 virtual machine where we have 1 virtual machine as driver and multiple virtual machine as
worker and combinely we call this as 1 VM.

Cluster types - There are two types of clusters

All purpose

    Created Manually
    Persistent - Means at any point of time we can stop the cluster we can re start the cluster.
    Suitable for interactive workloads
    Shared among many users
    expensive to run

Job cluster

    Created by jobs
    Terminated at the end of jobs
    suitable for automated workloads
    isolated just for the jobs
    cheaper to run

Cluster configuration
cluster mode - there are two types of cluster mode
    1) Multi Node - where we have one vm driver and multiple vm as worker
    2) Single Node - Where we have a single VM whenever request will come it behave as driver then later it
        behave as worker.

Access Mode -

    1) Single User - Only one user access, supports python, SQL, Scala, R
    2) Shared User - Multiple User access, only available in premium, supports Python, SQL, Scala, R
    3) No Isolation Shared - Multiple user access, Supports python, SQL, Scala, R

Databricks Runtime -

    1) Databricks Runtime - Spark, Scala, R, Python, SQL, Ubuntu libraries, GPU Libraries, Delta lake
    2) Databricks runtime ML - Everything from databricks runtime, Popular ML libraries(pytorch, keras,
        TensorFlow, XGBoost etc.)
    3) Databricks runtime genomics - Everything from databricks runtime, popular open source genomics lib(
        glow, Adam, etc), genomics pipelines(DNAseq, RNAseq etc)
    4) Databricks runtime light - runtime option only for jobs not requiring advances features.

Auto termination - Terminates the cluster after x minutes of inactivity.

Auto Scaling -
    User specifies the min and max work nodes.
    Auto scales between min and max based on workloads.
    Not recommended for streaming workloads.

Cluster VM Type/Size -

    1) Memory optimized
    2) Compute optimized
    3) Storage optimized
    4) General purpose
    5) GPU Accelerated

Databricks file system (DBFS)

DBFS is a distributed file system mounted into an azure databricks workspace and available on azure databricks
cluster. DBFS is an abstraction on top of scalable object storage.

The default storage location in DBFS is known as DBFS root.

    1) /FileStore - imported datafiles, generated plots, uploaded libraries.
    2) /databricks-datasets - Sample public datasets.
    3) /databricks-results - Files generated by downloading the full results of a query.

Databricks utilities(dbutils)

    1) File system utilities
    2) Data utilities
    3) Notebook workflow utilities
    4) Widgets utilities
    5) Secret utilities

File system utilities - The file system utility allows you to access databricks file system(DBFS), making it
easier to use Azure databricks as a file system. To list all the available commands, run dbutils.fs.help()

commands -
1) CP(from: string, to: String, recurse: boolean=false)
    boolean: copies a file or directory, possibly across FileSystems
2) head(file: String, maxBytes: int=65536)
    String: Returns upto the first 'maxBytes' bytes of the given file as a string encoded in UTF-8
3) ls(dir: String): Seq - list the contents of a directory.
4) mkdirs(dir: String): boolean - Creates the given directory if it does not exist, also creating any
    necessary parent directories.
5) mv(from: String, to:String, recurse:boolean=false)
    Boolean - moves a file or directory, possibly across filesystems
6) put(file: String, contents: String, overwrite: boolean=false)
    Boolean - Writes the given string out to a file, encoded in utf-8
7) rm(dir: String, recurse: boolean=false)
    Boolean - Removes a file or directory.

Mounting Azure Storage -
When we store data into DBFS then there is no connection required to access those files from notebooks.
But when we store data outside of DBFS such as Azure blob storage, Azure deta lake gen1, Azure deta lake gen2
then we need a mounting to access those data.

Create Mount point in Azure databricks
Mounts a specified source directory into DBFS at the specified mount point.
Mount points can be created either using account key or SAS token.

Using Account key

dbutils.fs.mount(
    source='wasbs://<container_name>@<storage_name>.blob.core.windows.net/',
    mount_point='/mnt/<mountName>',
    extra_configs={'fs.azure.account.key.<StorageAccountName>.blob.core.windows.net':'<AccountKey>'}
)

Where container_name is the name of the container
storage_name is the name of storage account
mountName is the name of mountpoint name by which we wanted to create mount
StorageAccountName is the name of storage account
AccountKey you will get this account key inside Security + networking > access keys and you can use
key1 or key2

Using SAS Token

dbutils.fs.mount(
    source='wasbs://<container_name>@<storage_name>.blob.core.window.net/',
    mount_point='/mnt/<mountName>',
    extra_configs={'fs.azure.sas.<ContainerName>.<StorageAccountName>.blob.core.window.net':'<AccountKey>'}
)

to delete the mount point
dbutils.fs.unmount('<path>')

dbutils.fs.mounts() - displays information about the mounted points in DBFS.
dbutils.fs.unmount(<MountPointPath>) - commands can be used to unmount mount points.
dbutils.fs.refreshMounts() - Forces all machines in this cluster to refresh their mount cache, ensuring they
receive the most recent information.
dbutils.fs.updateMount(<MountPointPath>) - Similar to mount(), but updates an existing mount point(if present)
instead of creating a new one. Returns an error if mount point not available.

How to connect azure SQL database in databricks and how to access SQL table in the notebook?

We can access azure SQL database in databricks notebook using JDBC connection.

driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
database_host = "provide_value"
database_port = "1433"
database_name = "provide_value"
table = "provide_value"
user = "provide_value"
password = "provide_value"
url = f"jdbc:sqlserver://{database_host}:{database_port};database={database_name}"

Table_Val = (
    spark.read.format("jdbc").option("driver", driver).option("url", url).option("dbtable", table)
    .option("user", user).option("password", password).load()
)

What is widgets utility in azure databricks? How to create parameter in notebook in azure databricks?

The widgets utility allows you to parameterize notebooks

commands -
Combobox
Dropdown
Text
Multiselect

Above four commands are used to create parameter.

get
getArgument

Above two commands are used to get the value of arguments.

remove
removeAll

Above two commands are used to remove the arguments.

What is the difference between get and getArgument method in widgets?
Both the method get and getArgument are used to fetch or read the value of widgets parameter.
The main difference between both is in get if specific parameter name which we wanted to read is not available
in the notebook then it will crash.

But in getArgument we can pass a second parameter as an else value as string type.
If specific paramater name does not exist in the notebook then second param will return.

We can also create widget with help of SQL command

Create Widget DropDown CountryDD Default 'India' Choices

select Country from (
  select 'Iceland' as Country union all select 'India' union all select 'France' union all select 'Finland'
)

create widget ComboBox CountryCB default 'Australia' choices
select Country from (
  select 'Iceland' as Country union all select 'India' union all select 'France' union all select 'Finland'
)

create widget MultiSelect CountryMS default 'India' choices
select Country from (
  select 'Iceland' as Country union all select 'India' union all select 'France' union all select 'Finland' union all select 'Australia'
)

create widget Text CountryTxt default 'India'

We can write display(df) ie python code inside SQL notebook also with help of magic command

%python

display(df)

%python
df.createOrReplaceTempView('country')

%python
dbutils.widgets.removeAll()

create widget ComboBox CountryCB default 'Australia' choices
select column_name from country;

We can read the value of widget parameter from SQL notebook also
select getArgument('CountryDD')

select * from CountryData where Location = getArgument('CountryDD');

select explode(split(getArgument('CountryMS'), ',', 0))

select * from CountryData where Location in (select explode(split(getArgument('CountryMS'), ',', 0)))

Earlier whenever we were establish the connection with file system other than DBFS we used account key
and that account key we use to write in notebook only that is not safe, Anyone can go and hack that.

To overcome with that situation we will have secret utilities and we store secret things in secret scopes.

A workspace is limited to a maximum of 100 secrets scope.

There are two types of secret scopes.
1) Azure key vault backed scope.
2) Databricks backed scope.

There is no specific url available in databricks to create secret scope.
we have a url where you can create secret scope.

https://<databricks-instance>#secrets/createScope

We can add access policy in azure key vault to access azure databricks.

There are three levels if permission.

1) MANAGE - Allowed to change ACLs, and read and write to this secret scope.
2) WRITE - Allowed to read and write the secret scope.
3) READ - Allowed to read the secret scope and list what secrets are available.


