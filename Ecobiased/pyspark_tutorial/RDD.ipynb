{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65243ffe-a922-4bd7-9f56-b96fd5a4bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4285108-bea6-4259-996d-1ce81ce6d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD - RDD stands for resilient distributed dataset. It is a fundamental data structure in apache spark and represent an immutable\n",
    "# distributed collection of objects that can be process in parallel accross a cluster.\n",
    "\n",
    "# Key Features of RDD \n",
    "\n",
    "# 1) Resilient - RDDs are fault tolerant, meaning if any partition of data is lost, spark can automatically recompute it using the lineage\n",
    "# (The sequence of transformations applied to the data.)\n",
    "\n",
    "# 2) Distributed - RDDs are divided into partitions that are distributed across multiple nodes in a cluster, allowing paralled processing.\n",
    "\n",
    "# 3) Dataset - RDD are a collection of records or objects, like a dataset that can be stored and manipulated in memory or disk.\n",
    "\n",
    "\n",
    "# How to create a RDD\n",
    "\n",
    "# 1) Paralleizing an existing collection (list) - \n",
    "# 2) Loading an external dataset like from HDD, S3, or a local system file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85635273-c855-4c98-a5d0-8d2e239284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92ed524-9c09-469c-a044-884fb0d51248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ashish\n"
     ]
    }
   ],
   "source": [
    "print(\"ashish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1397d8-9b04-4d05-ad07-dbd55a6abeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ca321-5ba7-4b1a-872e-19a774a1f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = spark.SparkContext.parallelize([('Mumbai', 1), ('Delhi', 2), ('Chennai', 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b47ee-3634-427e-aa05-6154ce0297b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds\n",
    "# Here it will not print the data because data is there inside memory. To print the data first you need to bring data into hard disk\n",
    "# we have collect method to print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4878d30-6138-4a47-b367-e766a643a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb8ac0-026e-4d11-8f0b-1ede4fa02b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59818d04-5213-44d6-8ae1-250b4d23076c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260768f-ad9c-482f-bd16-53c2bf582431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
